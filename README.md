Для запуска метода требуется склонировать репозиторий из GitHub по ссылке https://github.com/alexkat20/POI_project и перейти в папку Airflow, где будут лежать все скрипты для сбора и обработки данных. В главной директории репозитория представлены все файлы, которые можно использовать для работы с точками общественного интереса. Здесь имеются Jupyter ноутбуки для анализа и обработки результатов, парсеры карт и ноутбуки с пайплайнами в Kubeflow pipelines и Prefect. Они были сделаны для изучения альтернативы Airflow

Перед началом работы требуется запустить wsl на машине с Windows или сразу использовать Ubuntu. Для перехода в дистрибутив Ubuntu на wsl используется команда wsl –d Ubuntu. Как было указано ранее, для разворачивания окружения используется Docker Compose, в котором описаны все сервисы и инструменты: в папке airflow создан файл docker-compose.yaml, а также Dockerfile для сборки базового образа airflow и установки необходимых зависимостей и библиотек

Для работы с Docker Compose необходимо наличие Docker на компьютере. Необходимо включить wsl интеграцию с дистрибутивом Ubuntu, который был ранее поставлен. После запуска докера необходимо в wsl выполнить команду docker-compose build, и начнется сборка необходимых образов для работы сервисов. 
После успешной сборки образов необходимо выполнить команду docker-compose up, и начнется разворачивание всего окружения для работы над точками общественного интереса

Далее в интерфейсе Docker Desktop можно увидеть все запущенные сервисы. Здесь можно увидеть, по каким адресам открыты сервисы (вкладка Ports), и как их можно открыть

После запуска всех контейнеров можно открыть по ссылке localhost:8080 (если окружение разворачивается локально) инструмент Airflow и проверить, что он работает. При первом запуске нужно ввести логин и пароль

Все остальные сервисы будут доступны по тому же адресу, но другим портам, которые прописаны в интерфейсе Docker Desktop.
В папке airflow репозитория POI_project также находятся следующие папки:
1)	config для конфигурационных файлов;
2)	dags со скриптами для DAG’ов;
3)	logs для сохранения логов выполнения скриптов;
4)	plugins для установленных плагинов.

В директории dags находятся 4 файла для разных задач: Yandex_parser – скрипт для парсинга карт, ingest_data – DAG для сбора и предобработки, process_data – DAG для обработки данных и выделения точек общественного интереса, ключевых слов и эмоций, remove_emoji – скрипт для удаления графической информации из текстов. 
DAG process_data напрямую связан с приложением, которое развернулось под названием models-server. Это серверное приложение, написанное на FastApi для деплоя и применения NLP моделей RuBert, KeyBert и Stanza. В папке app находятся файлы для разворачивания приложения: Dockerfile со слоями установки нужных библиотек и базовым образом, main.py – само приложение со всеми ендпоинтами и зависимостями.

Последняя папка, необходимая для работы, — data, в которую складываются дампы базы данных. 
В репозитории также присутствуют юпитер ноутбуки с произведенным анализом данных в качестве примера. В ноутбуках были выполнены все те же самые шаги, что и в пайплайнах для сбора и обработки данных. Также в этих ноутбуках были проверены и отфильтрованы данные, полученные после работы метода

